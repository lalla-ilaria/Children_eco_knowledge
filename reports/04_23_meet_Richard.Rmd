---
title: "meeting Richard 04.23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rethinking)
library(rlist)

d <- list.load("../2_Data_preparation/processed_data.RData")

#loads simulation function
if( !exists( "simulation", mode = "function")) source( "../1_Simulation/1_simulation_knowledge.R" )


```

## AGE EFFECT
Run ordered categorical model without 56y old man, same results:
-no effect of age bA;
-linear effect of age from 0 to 26, no sign of lack of data from 0 to 5

```{r ord age no saalum, cache=TRUE}

dat <- list( D = 1,
             N = d$N -1, 
             L = d$L , 
             Q = d$Q ,    #n questionnaire items
             R = d$R ,    #n image recognition items
             A = d$A [d$A <= 50] , #round age
             Y_l = d$Y_l [rownames(d$Y_l) != "19586",] ,
             Y_q = d$Y_q [rownames(d$Y_q) != "19586",] , #answers questionnaire
             Y_r = d$Y_r [rownames(d$Y_r) != "19586",] , #answers picture recognition
             O = length (0 : 26 ) ,
             alpha = rep( 2, length (0:26 ) -1 ) )

m_ord <- stan( file =  "../models/2_model_code_ord_age.stan", data=dat , chains=3, cores=3)

precis(m_ord, pars = "bA")
```

```{r plot deltas}
#extract samples
post <- extract.samples(m_ord)

#explore age effects
plot (precis(m_ord, 2, pars = "delta_j"), main = "all ages same effect")

year_eff <- apply(post$delta_j, 1, cumsum)
plot(1:nrow(year_eff), year_eff[,1], type = "l", xlab = "Age", ylab = "Age specific effect on knowledge" )
for (i in 1:50) {
  lines(1:nrow(year_eff), year_eff[,i], type = "l", col = col.alpha( 'black', alpha = 0.1))
}


```

#simulate with missing ages
So I run the same model on simulated data measuring effect all the way to 50 years, with sigmoid age relation within the known range. 
The age specific effect keep growing after the data are not available anymore, but there is a clear difference between the deltas for ages where the increase of knowledge is more marked and where there is no information. When plotting the expected total effect at each age, there is no sign anymore of the sigmoid relation of knowledge to age at the beginning of the curve.
But importantly, the total effect of age bA is very strong.

```{r simulated to 50, cache=TRUE}
sim_data <- simulation( N = 100, M = 300, beta_A = 3, age_eff = "sigmoid")
#with each year a step
dat <- list( D = sim_data$n_dimensions,
             N = sim_data$N , 
             L = sim_data$M , 
             Q = 50,         #n questionnaire items
             R = sim_data$M/2,#n image recognition items
             A = round (sim_data$A) , #round age
             S = sim_data$S,
             Y_l = sim_data$Y ,
             Y_q = sim_data$Y[,1:50] ,           #answers questionnaire
             Y_r = sim_data$Y[,1:(sim_data$M/2)],  #answers picture recognition
             O = length (0 : 50 ),
             alpha = rep( 2, length (1:50) )
             )
m_ord_s <- stan( file =  "../models/2_model_code_ord_age.stan", data=dat , chains=3, cores=3)
```

```{r plot simulated data}
precis(m_ord_s, pars = "bA")


#extract samples
post <- extract.samples(m_ord_s)

#explore age effects
plot (precis(m_ord_s, 2, pars = "delta_j"), main = "Some ages have much higher effect")

year_eff <- apply(post$delta_j, 1, cumsum)
plot(1:nrow(year_eff), year_eff[,1], type = "l", xlab = "Age", ylab = "Age specific effect on knowledge" )
for (i in 1:50) {
  lines(1:nrow(year_eff), year_eff[,i], type = "l", col = col.alpha( 'black', alpha = 0.1))
}

```

#analyze with linear age 
I also tried to run a model with linear effect of age on the real data, and in this case the effect of age is definitely positive.

```{r linear age, cache=TRUE, cache.lazy = FALSE}
#continuous age
dat <- list( D = 1,
             N = d$N -1,
             L = d$L ,
             Q = d$Q ,    #n questionnaire items
             R = d$R ,    #n image recognition items
             A = standardize( d$A [d$A <= 50] ) , #round age
             Y_l = d$Y_l [rownames(d$Y_l) != "19586",] ,
             Y_q = d$Y_q [rownames(d$Y_q) != "19586",] , #answers questionnaire
             Y_r = d$Y_r [rownames(d$Y_r) != "19586",]  #answers picture recognition
              )

m_lin <- stan( file =  "../models/model_code_age.stan", data=dat , chains=3, cores=3)

precis(m_lin, pars = "bA")
```
This obviously emerges when comparing the individual effects aK on knowledge from the models with linear vs ordered categorical effect of age. In the ordered categorical model, since age has no effect, aK captures all the variation between ages and correlates with age itself. In the linear model, bA captures most of the effect of age and aK takes care of the individual variation, and does not correlate stronglu with age:

```{r age and aK}
par( mfrow = c( 1,2))

post <- extract.samples(m_ord)
aKs <- apply(post$aK, 2, mean)
plot(d$A [ d$A <= 50 ], aKs, xlab = "Age", ylab = "Individual effect on knowledge", main = "Ordered categorical")

post <- extract.samples(m_lin)
aKs <- apply(post$aK, 2, mean)
plot(d$A [ d$A <= 50 ], aKs, xlab = "Age", ylab = "Individual effect on knowledge", main = "Linear")
```

## DIMENSIONS
First thing I did Monday morning was to check what was going on with the dimensions. But the model was not recognizing multiple dimensions anymore. It turns out that when the data from freelists, questions and image recognition are analyzed together, the model with only one dimension is preferred. 

```{r dimensions everything together, cache=TRUE, cache.lazy = FALSE}
#all types of data
D <- c(1:3)
m_da <- list()
#run the model with 1:3 number of dimensions
for (i in 1:length(D)) {
  dat <- list( D = D[i],    #loop through dimensions
               N = d$N ,    #n of individuals 
               L = d$L ,    #n questionnaire items
               Q = d$Q ,    #n questionnaire items
               R = d$R ,    #n image recognition items
               A = standardize(d$A) , #standardized age
               Y_l = d$Y_l , #answers freelist
               Y_q = d$Y_q , #answers questionnaire
               Y_r = d$Y_r   #answers picture recognition
               
  )
  
  m_da[[i]] <- stan( file = "../models/1_dimensions_intercept_only_all_items.stan", data=dat , chains=3, cores=3, init = 0 )
}
#model comparison
compare(m_da[[1]], m_da[[2]], m_da[[3]])
```

It turns out that only the freelist and image recognition data have information on multiple dimensions.

```{r dimensions questions, cache=TRUE}
#questions only
m_dq <- list()
#run the model with 1:3 number of dimensions
for (i in 1:length(D)) {
  dat <- list( D = D[i],    #loop through dimensions
               N = d$N ,    #n of individuals
               Q = d$Q ,    #n questionnaire items
               A = standardize(d$A) , #standardized age
               S = ifelse(d$S == "m", 1, 2),
               Y_q = d$Y_q  #answers freelist
  )

  m_dq[[i]] <- stan( file = "../models/1_dimensions_age_questions_only.stan", data=dat , chains=3, cores=3, init = 0 )
}
#model comparison
compare(m_dq[[1]], m_dq[[2]], m_dq[[3]])
```

```{r dimensions recognition, cache=TRUE}
#recognition only
m_dr <- list()
#run the model with 1:3 number of dimensions
for (i in 1:length(D)) {
  dat <- list( D = D[i],    #loop through dimensions
               N = d$N ,    #n of individuals
               R = d$R ,    #n questionnaire items
               A = standardize(d$A) , #standardized age
               S = ifelse(d$S == "m", 1, 2),
               Y_r = d$Y_r  #answers freelist
  )

  m_dr[[i]] <- stan( file = "../models/1_dimensions_age_recognition_only.stan", data=dat , chains=3, cores=3, init = 0 )
}
#model comparison
compare(m_dr[[1]], m_dr[[2]], m_dr[[3]])

```

```{r dimensions freelist, cache=TRUE, cache.lazy = FALSE}
#freelist only
D <- c(1:3)
m_dl <- list()
#run the model with 1:3 number of dimensions
for (i in 1:length(D)) {
  dat <- list( D = D[i],    #loop through dimensions
               N = d$N ,    #n of individuals 
               L = d$L ,    #n questionnaire items
               A = standardize(d$A) , #standardized age
               S = ifelse(d$S == "m", 1, 2),
               Y_l = d$Y_l  #answers freelist
  )
  
  m_dl[[i]] <- stan( file = "../models/1_dimensions_age_freelist_only.stan", data=dat , chains=3, cores=3, init = 0 )
}
#model comparison
comparison <- compare(m_dl[[1]], m_dl[[2]], m_dl[[3]])

comparison

plot(comparison)

```

First, let's focus on the results of the model using only the freelist data and, for now, three dimensions.

To better unedrstand how the different dimensions capture variation among individuals in knowledge, let's plot the estimated knowledge K in each dimension against all other dimensions.


In this model we have intercepts for individuals and a slope for age. Two dimensions seem to do most of the job of describing knowledge, and the third correlates quite closely with both of the first two.

```{r lets see only age effect, cache=TRUE, cache.lazy = FALSE}
#freelist only
#run the model with 1:3 number of dimensions
  dat <- list( D = 3,    #loop through dimensions
               N = d$N ,    #n of individuals 
               L = d$L ,    #n questionnaire items
               A = standardize(d$A) , #standardized age
               Y_l = d$Y_l  #answers freelist
  )
  
  m_d3 <- stan( file = "../models/1_dimensions_age_freelist_only.stan", data=dat , chains=3, cores=3, init = 0 )

post_d <- extract.samples(m_d3)

```

```{r  dimensions in freelists no predictors knowledge}
post_d <- extract.samples(m_d3)

#check the correlation between dimensions
par( mfrow = c( 1,3))
#knowledge
plot(apply( post_d$K[,,1], 2, mean), apply( post_d$K[,,2], 2, mean),
     xlab = "K 1", ylab = "K 2")
plot(apply( post_d$K[,,1], 2, mean), apply( post_d$K[,,3], 2, mean),
     xlab = "K 1", ylab = "K 3")
plot(apply( post_d$K[,,2], 2, mean), apply( post_d$K[,,3], 2, mean),
     xlab = "K 2", ylab = "K 3")
```

In this model I introduced an intercept for sex. Interestingly, the relation between the dimensions changes. 

```{r  dimensions in freelists knowledge}
post_d <- extract.samples(m_dl[[3]])

#check the correlation between dimensions
par( mfrow = c( 1,3))
#knowledge
plot(apply( post_d$K[,,1], 2, mean), apply( post_d$K[,,2], 2, mean),
     xlab = "K 1", ylab = "K 2")
plot(apply( post_d$K[,,1], 2, mean), apply( post_d$K[,,3], 2, mean),
     xlab = "K 1", ylab = "K 3")
plot(apply( post_d$K[,,2], 2, mean), apply( post_d$K[,,3], 2, mean),
     xlab = "K 2", ylab = "K 3")
```

Let's see the effect of age and sex. Age seem to correlate positively with knowledge in all dimensions, but when sex is not included it does affect more one than the other dimensions. Compared with the model with no dimensions, age seems to be less important, though. 

```{r dimensions and effects of age}
precis(m_dl[[3]], 2, pars = "bA")
precis(m_d3, 2, pars = "bA")

par( mfrow = c( 1,2))
plot(precis(m_d3, 2, pars = "bA"), main = "age only")
plot(precis(m_dl[[3]], 2, pars = "bA"), main = "sex and age")
```



```{r dimensions and effects of sex}
precis(m_dl[[3]], 3, pars = "aS")

post_d$diff_fm <- post_d$aS[,1,] - post_d$aS[,2,]
precis(post_d, 2, pars = "diff_fm")

```

Another thing - the model is obv mixing horribly. But we were expecing it, right? Can we trust it?

```{r horrible mixing}
tracerplot(m_dl[[3]], pars = c ("bA", "aS"))
```




Now let's have a look at the item parameters
```{r dimensions in freelists items}
d$color_l <- ifelse( is.na(d$type_l), "yellow",
             ifelse( d$type_l  == "N", "darkorange",
             ifelse( d$type_l  == "S", "darkblue",
             ifelse( d$type_l  == "W", "darkred",
             ifelse( d$type_l  == "D", "orchid",
             ifelse( d$type_l  == "M", "darkgreen", 
             NA))))))

#discrimination
par( mfrow = c( 1,3))
plot(apply( post_d$a_l[,,1], 2, mean), apply( post_d$a_l[,,2], 2, mean),
     xlab = "discrimination 1", ylab = "discrimination 2",                     col = d$color_l)
plot(apply( post_d$a_l[,,1], 2, mean), apply( post_d$a_l[,,3], 2, mean),
     xlab = "discrimination 1", ylab = "discrimination 3",                     col = d$color_l)
plot(apply( post_d$a_l[,,2], 2, mean), apply( post_d$a_l[,,3], 2, mean),
     xlab = "discrimination 2", ylab = "discrimination 3",                     col = d$color_l)
#difficulty
par( mfrow = c( 1,3))
plot(apply( post_d$b_l[,,1], 2, mean), apply( post_d$b_l[,,2], 2, mean),
     xlab = "difficulty 1", ylab = "difficulty 2",                     
     col = d$color_l)
plot(apply( post_d$b_l[,,1], 2, mean), apply( post_d$b_l[,,3], 2, mean),
     xlab = "difficulty 1", ylab = "difficulty 3",                     
     col = d$color_l)
plot(apply( post_d$b_l[,,2], 2, mean), apply( post_d$b_l[,,3], 2, mean),
     xlab = "difficulty 2", ylab = "difficulty 3",                     
     col = d$color_l)
```

Seems interesting that the items that are hard are hard in all dimensions, where the easy ones vary by dimension -  which makes sense, cause even when named, the hard items are named only once, whereas the simple items named many times give space to variation.

Let's see how the items parameters interact with knowledge.
```{r knowledge on items colored}
post_d <- extract.samples(m_dl[[3]])
par( mfrow = c( 1,3))

#1st dimension
Ks_1 <- apply(post_d$K[,,1], 2, mean)
a_ls_1 <- apply(post_d$a_l[,,1], 2, mean)
b_ls_1 <- apply(post_d$b_l[,,1], 2, mean)
curve(inv_logit(a_ls_1[1] * ( x - b_ls_1[1])), 
      xlim = c(-7, 5), ylim = c(0, 1), 
      xlab = "knowledge dimension 1", ylab = "p correct answer",
      col = col.alpha( d$color_l, 0.2))
for(i in 1: length(a_ls_1)){
  curve(inv_logit(a_ls_1[i] * ( x - b_ls_1[i])), 
        col = col.alpha(d$color_l[i], 0.2), add = TRUE)
}
points(Ks_1, rep(0.5, length(Ks_1)), col = col.alpha("cornflowerblue", 0.7))  

#2nd dimension
Ks_2 <- apply(post_d$K[,,2], 2, mean)
a_ls_2 <- apply(post_d$a_l[,,2], 2, mean)
b_ls_2 <- apply(post_d$b_l[,,2], 2, mean)
curve(inv_logit(a_ls_2[1] * ( x - b_ls_2[1])), 
      xlim = c(-7, 5), ylim = c(0, 1), 
      xlab = "knowledge dimension 2", ylab = "p correct answer",
      col = col.alpha(d$color_l, 0.2))
for(i in 1: length(a_ls_2)){
  curve(inv_logit(a_ls_2[i] * ( x - b_ls_2[i])), 
        col = col.alpha(d$color_l[i], 0.2), add = TRUE)
}
points(Ks_2, rep(0.5, length(Ks_2)),col = col.alpha("cornflowerblue", 0.7))  

#3rd dimension
Ks_3 <- apply(post_d$K[,,3], 2, mean)
a_ls_3 <- apply(post_d$a_l[,,3], 2, mean)
b_ls_3 <- apply(post_d$b_l[,,3], 2, mean)
curve(inv_logit(a_ls_3[1] * ( x - b_ls_3[1])), 
      xlim = c(-7, 5), ylim = c(0, 1), 
      xlab = "knowledge dimension 3", ylab = "p correct answer",
      col = col.alpha(d$color_l, 0.2))
for(i in 1: length(a_ls_3)){
  curve(inv_logit(a_ls_3[i] * ( x - b_ls_3[i])), 
        col = col.alpha(d$color_l[i], 0.2), add = TRUE)
}
points(Ks_3, rep(0.5, length(Ks_3)),col = col.alpha("cornflowerblue", 0.7))  
```
